import csv
import os
import re
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from random import sample
from sys import argv
from typing import cast

import niquests
import zhconv
from slugify import slugify

# ================== é…ç½® ==================
ARCHIVE_TIME = '2023-01-01 00:00:00'

QUESTIONNAIRE: list[str] = [
    'å®¿èˆæ˜¯ä¸ŠåºŠä¸‹æ¡Œå—ï¼Ÿ',
    'æ•™å®¤å’Œå®¿èˆæœ‰æ²¡æœ‰ç©ºè°ƒï¼Ÿ',
    'æœ‰ç‹¬ç«‹å«æµ´å—ï¼Ÿæ²¡æœ‰ç‹¬ç«‹æµ´å®¤çš„è¯ï¼Œæ¾¡å ‚ç¦»å®¿èˆå¤šè¿œï¼Ÿ',
    'æœ‰æ—©è‡ªä¹ ã€æ™šè‡ªä¹ å—ï¼Ÿ',
    'æœ‰æ™¨è·‘å—ï¼Ÿ',
    'æ¯å­¦æœŸè·‘æ­¥æ‰“å¡çš„è¦æ±‚æ˜¯å¤šå°‘å…¬é‡Œï¼Œå¯ä»¥éª‘è½¦å—ï¼Ÿ',
    'å¯’æš‘å‡æ”¾å¤šä¹…ï¼Œæ¯å¹´å°å­¦æœŸæœ‰å¤šé•¿ï¼Ÿ',
    'å­¦æ ¡å…è®¸ç‚¹å¤–å–å—ï¼Œå–å¤–å–çš„åœ°æ–¹ç¦»å®¿èˆæ¥¼å¤šè¿œï¼Ÿ',
    'å­¦æ ¡äº¤é€šä¾¿åˆ©å—ï¼Œæœ‰åœ°é“å—ï¼Œåœ¨å¸‚åŒºå—ï¼Œä¸åœ¨çš„è¯è¿›åŸè¦å¤šä¹…ï¼Ÿ',
    'å®¿èˆæ¥¼æœ‰æ´—è¡£æœºå—ï¼Ÿ',
    'æ ¡å›­ç½‘æ€ä¹ˆæ ·ï¼Ÿ',
    'æ¯å¤©æ–­ç”µæ–­ç½‘å—ï¼Œå‡ ç‚¹å¼€å§‹æ–­ï¼Ÿ',
    'é£Ÿå ‚ä»·æ ¼è´µå—ï¼Œä¼šåƒå‡ºå¼‚ç‰©å—ï¼Ÿ',
    'æ´—æ¾¡çƒ­æ°´ä¾›åº”æ—¶é—´ï¼Ÿ',
    'æ ¡å›­å†…å¯ä»¥éª‘ç”µç“¶è½¦å—ï¼Œç”µæ± åœ¨å“ªèƒ½å……ç”µï¼Ÿ',
    'å®¿èˆé™ç”µæƒ…å†µï¼Ÿ',
    'é€šå®µè‡ªä¹ æœ‰å»å¤„å—ï¼Ÿ',
    'å¤§ä¸€èƒ½å¸¦ç”µè„‘å—ï¼Ÿ',
    'å­¦æ ¡é‡Œé¢ç”¨ä»€ä¹ˆå¡ï¼Œé¥­å ‚æ€æ ·æ¶ˆè´¹ï¼Ÿ',
    'å­¦æ ¡ä¼šç»™å­¦ç”Ÿå‘é“¶è¡Œå¡å—ï¼Ÿ',
    'å­¦æ ¡çš„è¶…å¸‚æ€ä¹ˆæ ·ï¼Ÿ',
    'å­¦æ ¡çš„æ”¶å‘å¿«é€’æ”¿ç­–æ€ä¹ˆæ ·ï¼Ÿ',
    'å­¦æ ¡é‡Œé¢çš„å…±äº«å•è½¦æ•°ç›®ä¸ç§ç±»å¦‚ä½•ï¼Ÿ',
    'ç°é˜¶æ®µå­¦æ ¡çš„é—¨ç¦æƒ…å†µå¦‚ä½•ï¼Ÿ',
    'å®¿èˆæ™šä¸ŠæŸ¥å¯å—ï¼Œå°å¯å—ï¼Œæ™šå½’èƒ½å›å»å—ï¼Ÿ',
]

NAME_PREPROCESS = re.compile(r'[\(\)ï¼ˆï¼‰ã€ã€‘#]')
FILENAME_PREPROCESS = re.compile(r'[/>|:&]')
NORMAL_NAME_MATCHER = re.compile(r'å¤§å­¦|å­¦é™¢|å­¦æ ¡')

ROOT = Path('required')
SITE_DIR = Path(r'D:\Project\questionnaire-report-theme')

REQUIRED_FILES = [
    'README_archived_template.md',
    'README_template.md',
    'alias.txt',
    'blacklist.txt',
    'colleges.csv',
    'results_desensitized.csv',
    'whitelist.txt',
]

REQUIRED_DOCS = [
    'å‡ºå›½å—é˜».md',
    'å¦‚ä½•æ­£ä¹‰åŠé€€ï¼Ÿ.md',
    'å½±å“ç”Ÿæ´»è´¨é‡çš„ä¸€äº›æ–¹é¢.md',
]

BASE_URL = 'https://raw.githubusercontent.com/CollegesChat/university-information/refs/heads/master/questionnaires/'
DOC_URL = BASE_URL + 'site/docs/choose-a-college/'


# ================== æ•°æ®ç±» ==================
@dataclass
class IndexedContent:
    answer_id: int
    content: str

    def __str__(self) -> str:
        return f'A{self.answer_id}: {self.content}'


@dataclass
class AnswerGroup:
    answers: list[IndexedContent] = field(default_factory=list)

    def add_answer(self, ans: IndexedContent) -> None:
        self.answers.append(ans)

    def extend(self, other: 'AnswerGroup') -> None:
        self.answers.extend(other.answers)


@dataclass
class University:
    answers: list[AnswerGroup] = field(
        default_factory=lambda: [AnswerGroup() for _ in range(len(QUESTIONNAIRE))]
    )
    additional_answers: list[IndexedContent] = field(default_factory=list)
    credits: list[IndexedContent] = field(default_factory=list)

    def add_answer(self, index: int, answer: IndexedContent) -> None:
        self.answers[index].add_answer(answer)

    def add_additional_answer(self, answer: IndexedContent) -> None:
        if answer.content:
            self.additional_answers.append(answer)

    def add_credit(self, credit: IndexedContent) -> None:
        self.credits.append(credit)

    def combine_from(self, other: 'University') -> None:
        for mine, other_group in zip(self.answers, other.answers, strict=True):
            mine.extend(other_group)
        self.additional_answers.extend(other.additional_answers)
        self.credits.extend(other.credits)


class FilenameMap:
    def __init__(self) -> None:
        self.mapping: dict[str, str] = {}
        self.used: set[str] = set()

    def __getitem__(self, name: str) -> str:
        if name in self.mapping:
            return self.mapping[name]
        base = slugify(FILENAME_PREPROCESS.sub('', name))
        slug = base
        idx = 1
        while slug in self.used:
            idx += 1
            slug = f'{base}-{idx}'
        self.mapping[name] = slug
        self.used.add(slug)
        return slug


# ================== è¾…åŠ©å‡½æ•°ï¼ˆç®€ä½“ä¸­æ–‡æ³¨é‡Šï¼‰ ==================
def download_files(names: list[str], base_url: str, root: Path) -> None:
    """ä¸‹è½½ç¼ºå¤±çš„æ–‡ä»¶åˆ° root ç›®å½•"""
    root.mkdir(parents=True, exist_ok=True)
    for name in names:
        local_file = root / name
        if not local_file.exists():
            url = base_url + name
            print(f'Downloading {name} from {url}...')
            r = niquests.get(url)
            if r.status_code == 200:
                local_file.write_bytes(cast(bytes, r.content))
                print(f'Saved {name}')
            else:
                print(f'Failed to download {name}, status code: {r.status_code}')


def markdown_escape(text: str) -> str:
    return text.replace('*', '\\*').replace('~', '\\~').replace('_', '\\_')


def generate_markdown_path(province: str, university_name: str, archived: bool) -> Path:
    """ç”Ÿæˆ Hugo é¡µé¢ bundle çš„ç›®æ ‡è·¯å¾„"""
    base = SITE_DIR / 'content' / 'docs'
    if archived:
        base = base / 'archived'
    return base / 'universities' / province / f'{university_name}.md'


def load_colleges() -> tuple[dict[str, list], dict[str, str]]:
    provinces: dict[str, list] = {}
    colleges: dict[str, str] = {}
    csv_path = ROOT / 'colleges.csv'
    if not csv_path.exists():
        raise FileNotFoundError('colleges.csv not found')
    with csv_path.open('r', encoding='utf-8') as f:
        reader = csv.reader(f)
        for province, college in reader:
            key = NAME_PREPROCESS.sub('', college).replace(' ', '')
            colleges[key] = province
            provinces.setdefault(province, [])
    provinces.setdefault('å…¶ä»–', [])
    return provinces, colleges


def load_to_universities(universities: defaultdict[str, University], row: list) -> None:
    """æŠŠä¸€è¡Œé—®å·è®°å½•åŠ è½½åˆ° universities å­—å…¸"""
    aid, _, anonymous, email, show_email, name, *answers = row[:-9]
    additional_answer = IndexedContent(int(aid), row[-9])
    anonymous_flag = int(anonymous) == 2
    show_email_flag = not anonymous_flag and float(show_email) == 1.0
    name = zhconv.convert(name, 'zh-cn')
    name = NAME_PREPROCESS.sub('', name).strip()
    uni = universities[name]
    submit_time = datetime.strptime(row[-8], '%Y-%m-%d %H:%M:%S')
    credit_text = (
        f'{email} ({submit_time:%Y å¹´ %m æœˆ})'
        if show_email_flag and email
        else f'åŒ¿å ({submit_time:%Y å¹´ %m æœˆ})'
    )
    uni.add_credit(IndexedContent(int(aid), credit_text))
    for i, ans in enumerate(answers):
        uni.add_answer(i, IndexedContent(int(aid), ans))
    uni.add_additional_answer(additional_answer)


def process_universities(universities: dict, colleges: dict) -> None:
    """å¤„ç†åˆ«åã€é»‘åå•ä¸å¯èƒ½æ— æ•ˆçš„åç§°æç¤º"""
    alias_path = ROOT / 'alias.txt'
    if alias_path.exists():
        with alias_path.open('r', encoding='utf-8') as f:
            for line in f:
                name, *aliases = line.rstrip('\n').split('ğŸš®')
                primary = universities.get(name)
                if primary is None:
                    # Debug mode may only load a subset; skip missing primary names.
                    print(f'[warn] alias primary missing: {name}')
                    continue
                for alias in aliases:
                    if alias in universities:
                        primary.combine_from(universities[alias])
                        del universities[alias]
    blacklist = ROOT / 'blacklist.txt'
    if blacklist.exists():
        with blacklist.open('r', encoding='utf-8') as f:
            for line in f:
                universities.pop(line.strip(), None)
    wl = ROOT / 'whitelist.txt'
    whitelist = (
        {l.strip() for l in wl.read_text(encoding='utf-8').splitlines()}
        if wl.exists()
        else set()
    )
    for name in list(universities.keys()):
        if NORMAL_NAME_MATCHER.search(name) is None and name not in whitelist:
            print(
                f'[warn] maybe invalid: {name} '
                + ','.join(f'A{_.answer_id}' for _ in universities[name].credits)
            )


def ensure_dirs() -> None:
    (SITE_DIR / 'content' / 'docs' / 'universities').mkdir(parents=True, exist_ok=True)
    (SITE_DIR / 'content' / 'docs' / 'archived' / 'universities').mkdir(
        parents=True, exist_ok=True
    )
    (SITE_DIR / 'content' / 'docs' / 'choose-a-college').mkdir(
        parents=True, exist_ok=True
    )


def sanitize_filename(filename: str) -> tuple[str, bool]:
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦å¹¶åˆ¤æ–­æ˜¯å¦è¢«æ›¿æ¢"""
    illegal_pattern = r'[\\/:*?"<>|\0]'
    cleaned = re.sub(illegal_pattern, '_', str(filename))
    return cleaned, cleaned != filename


def find_province(name: str, colleges: dict[str, str]) -> str:
    for key, prov in colleges.items():
        if name.find(key) >= 0:
            return prov
    return 'å…¶ä»–'


def render_university_markdown(
    name: str, uni: University, slug: str, archived: bool
) -> str:
    lines: list[str] = [
        '---\n',
        f'title: "{name}{" (å·²å½’æ¡£)" if archived else ""}"\n',
        f'slug: "{slug}"\n',
        f'description: æ¥è‡ª colleges.chat çš„{name} é—®å·è°ƒæŸ¥ä¿¡æ¯\n',
        '---\n\n',
    ]
    lines.append('> æœ¬é¡µé¢å†…å®¹æ¥æºäºé—®å·ï¼Œä»…ä¾›å‚è€ƒã€‚\n\n')
    lines.append('> æ•°æ®æ¥æºï¼š\n<details><summary>å±•å¼€</summary>\n<ul>\n')
    for c in uni.credits:
        lines.append(f'<li>{c}</li>\n')
    lines.append('</ul>\n</details>\n\n')
    for q, group in zip(QUESTIONNAIRE, uni.answers, strict=True):
        lines.append(f'## Q: {q}\n\n')
        for ans in group.answers:
            lines.append(f'- {markdown_escape(str(ans))}\n')
    if uni.additional_answers:
        lines.append('\n## è‡ªç”±è¡¥å……\n\n')
        for a in uni.additional_answers:
            lines.append(markdown_escape(str(a)) + '\n\n')
    return ''.join(lines)


def write_university_markdown(
    name: str,
    uni: University,
    slug: str,
    target: Path,
    archived: bool,
) -> None:
    target.write_text(
        render_university_markdown(name, uni, slug, archived), encoding='utf-8'
    )


def write_markdown_for_universities(
    universities: dict[str, University],
    filename_map: FilenameMap,
    colleges: dict[str, str],
    archived: bool,
) -> None:
    """æŠŠ universities å¹¶å‘å†™æˆ Hugo çš„ markdown é¡µé¢"""
    tasks: list[tuple[str, University, str, Path]] = []
    for name, uni in universities.items():
        slug = filename_map[name]
        province = find_province(name, colleges)
        target = generate_markdown_path(province, name, archived)
        target_name, is_illegal = sanitize_filename(target.stem)
        if is_illegal:
            print(f'[error] {target} æ–‡ä»¶åå¯èƒ½éæ³•ï¼')
            target = target.with_stem(target_name)
        tasks.append((name, uni, slug, target))

    for parent in {target.parent for _, _, _, target in tasks}:
        parent.mkdir(parents=True, exist_ok=True)
        (parent / '_index.md').touch()

    max_workers = min(32, max(1, (os.cpu_count() or 1) * 4))
    section = 'archived' if archived else 'active'
    total = len(tasks)
    print(f'[info] Start generating {section} markdown files: {total}')
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(
                write_university_markdown, name, uni, slug, target, archived
            )
            for name, uni, slug, target in tasks
        ]
        completed = 0
        for future in as_completed(futures):
            future.result()
            completed += 1
            progress = completed / total * 100 if total else 100.0
            print(
                f'\r[progress] {section}: {completed}/{total} ({progress:.1f}%)',
                end='',
                flush=True,
            )
    print()


ensure_dirs()
download_files(REQUIRED_FILES, BASE_URL, ROOT)
download_files(
    REQUIRED_DOCS, DOC_URL, SITE_DIR / 'content' / 'docs' / 'choose-a-college'
)
download_files(['index.md'], BASE_URL + '/site/docs/', SITE_DIR / 'content' / 'docs')

target_file = SITE_DIR / 'content' / 'docs' / 'index.md'
new_file = target_file.with_name('_index.md')
target_file.rename(new_file)
header = '---\ntitle: é¦–é¡µ\nurl: /\n---\n\n'
content = new_file.read_text(encoding='utf-8')
new_file.write_text(header + content, encoding='utf-8')

provinces, colleges = load_colleges()
archive_cut = datetime.strptime(ARCHIVE_TIME, '%Y-%m-%d %H:%M:%S')

universities: defaultdict[str, University] = defaultdict(University)
universities_archived: defaultdict[str, University] = defaultdict(University)

with (ROOT / 'results_desensitized.csv').open('r', encoding='utf-8') as f:
    reader = csv.reader(f)
    next(reader, None)
    for row in reader:
        t = datetime.strptime(row[-8], '%Y-%m-%d %H:%M:%S')
        target = universities_archived if t < archive_cut else universities
        load_to_universities(target, row)

if 'debug' in argv:
    universities: dict[str, University] = dict(sample(list(universities.items()), 100))
    universities_archived: dict[str, University] = dict(
        sample(list(universities_archived.items()), 100)
    )
    print(
        f'Debug mode: only processing 100 universities each <{len(universities)} and {len(universities_archived)} >.'
    )
process_universities(universities, colleges)
process_universities(universities_archived, colleges)


write_markdown_for_universities(universities, FilenameMap(), colleges, archived=False)
write_markdown_for_universities(
    universities_archived, FilenameMap(), colleges, archived=True
)
